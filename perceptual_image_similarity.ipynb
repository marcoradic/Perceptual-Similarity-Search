{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.4'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook\n",
    "import lycon\n",
    "from lshash.lshash import LSHash\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['caltech_images/caltech101/gerenuk',\n",
       " 'caltech_images/caltech101/hawksbill',\n",
       " 'caltech_images/caltech101/headphone',\n",
       " 'caltech_images/caltech101/ant',\n",
       " 'caltech_images/caltech101/butterfly']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_PATH = Path('./caltech_images/caltech101/')\n",
    "\n",
    "glob.glob(str(DATA_PATH) + '/*')[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting the feature vectors from VGG16 (pretrained on ImageNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "data_generator = ImageDataGenerator(rescale=1./255,)\n",
    "net = keras.applications.VGG16(include_top=False, weights='imagenet')\n",
    "\n",
    "generator = data_generator.flow_from_directory(DATA_PATH,\n",
    "                                               target_size=(224, 224),\n",
    "                                               batch_size=batch_size,\n",
    "                                               class_mode=None,\n",
    "                                               shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature_vectors = net.predict_generator(generator, len(filenames) // batch_size, verbose=1) # use for inference (can take some time)\n",
    "#filenames = generator.filenames\n",
    "\n",
    "feature_vectors = np.load('./caltech_images/vgg16_features/features.vgg16.npy')\n",
    "with open(r\"./caltech_images/vgg16_features/filenames.pickle\", \"rb\") as input_file:\n",
    "    feature_filenames = pickle.load(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9088, 25088)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_vectors.shape\n",
    "feature_vectors = feature_vectors.reshape((feature_vectors.shape[0], -1))\n",
    "feature_vectors = normalize(feature_vectors, axis=1, norm='l2') # normalizing every feature vector\n",
    "feature_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_table = dict(zip(feature_filenames, feature_vectors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the Hashtable (lshash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "hash_index = LSHash(hash_size=16, input_dim=feature_vectors.shape[-1], num_hashtables=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dca683daaeba459b8b1dda55561432e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=9088), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for filename, feature_vec in tqdm_notebook(features_table.items()):\n",
    "    hash_index.index(feature_vec, extra_data=filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running queries for similar images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similar_images(index, results=10):\n",
    "    if isinstance(index, int):\n",
    "        response = hash_index.query(features_table[list(features_table.keys())[index]], \n",
    "                         num_results=11, distance_func='cosine')\n",
    "    else:\n",
    "        response = hash_index.query(features_table[index], \n",
    "                         num_results=11, distance_func='cosine')\n",
    "    images = list()\n",
    "    for i in range(1, 12):\n",
    "        img = lycon.load(str(DATA_PATH / response[i-1][0][1]))\n",
    "        images.append(str(DATA_PATH / response[i-1][0][1]))\n",
    "    imagesList = ''.join( [f\"<img style='height: 120px; margin: 0px; float: left; border: 1px solid black;' src='{s}' />\"\n",
    "                     for s in images ])\n",
    "    display(HTML(imagesList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img style='height: 120px; margin: 0px; float: left; border: 1px solid black;' src='caltech_images/caltech101/laptop/image_0027.jpg' /><img style='height: 120px; margin: 0px; float: left; border: 1px solid black;' src='caltech_images/caltech101/laptop/image_0041.jpg' /><img style='height: 120px; margin: 0px; float: left; border: 1px solid black;' src='caltech_images/caltech101/laptop/image_0058.jpg' /><img style='height: 120px; margin: 0px; float: left; border: 1px solid black;' src='caltech_images/caltech101/laptop/image_0013.jpg' /><img style='height: 120px; margin: 0px; float: left; border: 1px solid black;' src='caltech_images/caltech101/laptop/image_0070.jpg' /><img style='height: 120px; margin: 0px; float: left; border: 1px solid black;' src='caltech_images/caltech101/laptop/image_0076.jpg' /><img style='height: 120px; margin: 0px; float: left; border: 1px solid black;' src='caltech_images/caltech101/laptop/image_0012.jpg' /><img style='height: 120px; margin: 0px; float: left; border: 1px solid black;' src='caltech_images/caltech101/laptop/image_0022.jpg' /><img style='height: 120px; margin: 0px; float: left; border: 1px solid black;' src='caltech_images/caltech101/watch/image_0210.jpg' /><img style='height: 120px; margin: 0px; float: left; border: 1px solid black;' src='caltech_images/caltech101/pyramid/image_0048.jpg' /><img style='height: 120px; margin: 0px; float: left; border: 1px solid black;' src='caltech_images/caltech101/laptop/image_0033.jpg' />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "similar_images('laptop/image_0027.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img style='height: 120px; margin: 0px; float: left; border: 1px solid black;' src='caltech_images/caltech101/strawberry/image_0027.jpg' /><img style='height: 120px; margin: 0px; float: left; border: 1px solid black;' src='caltech_images/caltech101/dolphin/image_0017.jpg' /><img style='height: 120px; margin: 0px; float: left; border: 1px solid black;' src='caltech_images/caltech101/strawberry/image_0019.jpg' /><img style='height: 120px; margin: 0px; float: left; border: 1px solid black;' src='caltech_images/caltech101/bonsai/image_0122.jpg' /><img style='height: 120px; margin: 0px; float: left; border: 1px solid black;' src='caltech_images/caltech101/bonsai/image_0111.jpg' /><img style='height: 120px; margin: 0px; float: left; border: 1px solid black;' src='caltech_images/caltech101/BACKGROUND_Google/image_0260.jpg' /><img style='height: 120px; margin: 0px; float: left; border: 1px solid black;' src='caltech_images/caltech101/platypus/image_0025.jpg' /><img style='height: 120px; margin: 0px; float: left; border: 1px solid black;' src='caltech_images/caltech101/brain/image_0046.jpg' /><img style='height: 120px; margin: 0px; float: left; border: 1px solid black;' src='caltech_images/caltech101/strawberry/image_0006.jpg' /><img style='height: 120px; margin: 0px; float: left; border: 1px solid black;' src='caltech_images/caltech101/strawberry/image_0034.jpg' /><img style='height: 120px; margin: 0px; float: left; border: 1px solid black;' src='caltech_images/caltech101/strawberry/image_0011.jpg' />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "similar_images('strawberry/image_0027.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img style='height: 120px; margin: 0px; float: left; border: 1px solid black;' src='caltech_images/caltech101/water_lilly/image_0009.jpg' /><img style='height: 120px; margin: 0px; float: left; border: 1px solid black;' src='caltech_images/caltech101/water_lilly/image_0001.jpg' /><img style='height: 120px; margin: 0px; float: left; border: 1px solid black;' src='caltech_images/caltech101/lotus/image_0040.jpg' /><img style='height: 120px; margin: 0px; float: left; border: 1px solid black;' src='caltech_images/caltech101/lotus/image_0064.jpg' /><img style='height: 120px; margin: 0px; float: left; border: 1px solid black;' src='caltech_images/caltech101/sunflower/image_0037.jpg' /><img style='height: 120px; margin: 0px; float: left; border: 1px solid black;' src='caltech_images/caltech101/water_lilly/image_0018.jpg' /><img style='height: 120px; margin: 0px; float: left; border: 1px solid black;' src='caltech_images/caltech101/sunflower/image_0047.jpg' /><img style='height: 120px; margin: 0px; float: left; border: 1px solid black;' src='caltech_images/caltech101/sunflower/image_0036.jpg' /><img style='height: 120px; margin: 0px; float: left; border: 1px solid black;' src='caltech_images/caltech101/sunflower/image_0001.jpg' /><img style='height: 120px; margin: 0px; float: left; border: 1px solid black;' src='caltech_images/caltech101/water_lilly/image_0008.jpg' /><img style='height: 120px; margin: 0px; float: left; border: 1px solid black;' src='caltech_images/caltech101/water_lilly/image_0026.jpg' />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "similar_images('water_lilly/image_0009.jpg')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
